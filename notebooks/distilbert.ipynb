{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification,\\\nAdamW, get_linear_schedule_with_warmup\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Sampler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-20T17:29:49.776404Z","iopub.execute_input":"2023-03-20T17:29:49.776691Z","iopub.status.idle":"2023-03-20T17:30:00.358357Z","shell.execute_reply.started":"2023-03-20T17:29:49.776663Z","shell.execute_reply":"2023-03-20T17:30:00.357172Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Данные те же, что и в BertPreTraining**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/sentiment-analysis-company-reviews/train.csv')\ntest_df = pd.read_csv('/kaggle/input/sentiment-analysis-company-reviews/test.csv')\nsubmission=pd.read_csv('/kaggle/input/sentiment-analysis-company-reviews/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_df['Rating']\ntrain = train_df['Review']\ntest = test_df['Review']\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_dataset_torch(data: pd.DataFrame):\n    input_ids = []\n    attention_masks = []\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    \n    for row in tqdm(data, total=data.shape[0]):\n        encoded_dict = tokenizer.encode_plus(row, max_length=512,\n                                             pad_to_max_length=True,\n                                             return_attention_mask=True, \n                                             return_tensors='pt', \n                                             truncation=True,\n                                            )\n       \n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n        \n\n   \n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    input_ids.to(dtype=torch.long)\n    attention_masks.to(dtype=torch.long)\n    return input_ids, attention_masks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataprep(data: pd.DataFrame, batch_size: int, sampler: Sampler, labels=None): -> DataLoader\n        inps, masks = convert_to_dataset_torch(data)\n        if labels:\n            labels = torch.tensor(labels)\n            encoded = TensorDataset(inps, masks, labels)\n        else:\n            encoded = TensorDataset(inps, masks)\n        \n        dataloader = DataLoader(\n            encoded,  \n            sampler = sampler(encoded),\n            batch_size = batch_size,\n            num_workers = 0,\n            drop_last=True\n        )\n\n        return dataloader","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = dataprep(\n    data=train,\n    labels=y_train-1,\n    batch_size=16,\n    sampler=RandomSampler\n)\n\ntest_dataloader = dataprep(\n    data=test,\n    batch_size=16,\n    sampler=SequentialSampler\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_to_train = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n                                                           num_labels=5)\noptimizer = AdamW(model_to_train.parameters(),lr=2e-5,eps=1e-8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainloop(model, optimizer, dataloader, device, epochs):\n    model.to(device);\n    total_steps = len(dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)\n    for epoch_i in tqdm(range(0, epochs)):\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n\n        total_train_loss = 0\n\n        losses = []\n        model.train()\n\n        for step, batch in enumerate(dataloader):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            optimizer.zero_grad()        \n\n            loss = model(input_ids=b_input_ids, \n                                 attention_mask=b_input_mask, \n                                 labels=b_labels).loss\n\n            total_train_loss += loss.item()\n            losses.append(loss.item())\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n\n            scheduler.step()\n\n            if len(losses) == 20:\n                print(\"Loss: {}\".format(sum(losses)/len(losses)))\n                losses = []\n\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model = trainloop(\n    model=model_to_train,\n    optimizer=optimizer,\n    dataloader=train_dataloader,\n    device='cuda:0',\n    epochs=3\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(trained_model, 'distilbert3eps.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation(model, dataloader, device):\n    preds = []\n    for step, batch in enumerate(tqdm(dataloader)):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)  \n            preds.append(torch.argmax(torch.nn.Softmax()(model(input_ids=b_input_ids, \n                                 attention_mask=b_input_mask, \n                                 ).logits), dim=1).cpu().detach().numpy() + 1) # Метка класса\n    return np.array(preds).flatten()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = evaluation(\n    model=trained_model, \n    dataloader=test_dataloader, \n    device='cuda:0'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Rating'] = predictions\nsubmission.to_csv('distilbert3eps.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}